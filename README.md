Hello and welcome to my machine-learning portfolio. This is a complete collection of my work.
Algorithms covered (so far):

## Supervised Learning:

1. **Linear Regression**: A statistical method to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.

2. **Logistic Regression**: A regression analysis used for predicting the probability of a categorical dependent variable by fitting data to a logistic curve.

3. **Decision Trees**: A tree-like structure where an internal node represents a feature, the branches represent a decision rule, and each leaf node represents the outcome.

4. **Random Forest**: An ensemble learning method that builds multiple decision trees and merges their predictions to improve accuracy and reduce overfitting.

5. **K-Nearest Neighbors (KNN)**: A non-parametric algorithm used for classification and regression by finding the majority class among the k nearest data points.

6. **Gradient Boosting Machines (GBM)**: An ensemble learning technique that builds multiple weak learners sequentially, each correcting the errors of its predecessor.

7. **Neural Networks (Deep Learning)**: A computational model inspired by the structure and functioning of the human brain, consisting of interconnected layers of nodes that perform mathematical operations.

## Unsupervised Learning:

1. **K-Means Clustering**: An unsupervised learning algorithm that partitions data into clusters based on similarity, aiming to minimize the within-cluster sum of squares.

2. **Principal Component Analysis (PCA)**: A dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving the most important information.





